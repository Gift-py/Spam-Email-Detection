{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic algorithm\n",
    "#convert email into feature vector\n",
    "#add hyperparameters to:\n",
    "# - strip email headers\n",
    "# - convert to lowercase\n",
    "# - remove punctuation\n",
    "# - replace url with \"URL\" \n",
    "# - replace numbers with \"NUMBERS\"\n",
    "# - Preform Stemming (trim word endings with library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spam', 'ham']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import email\n",
    "import email.policy\n",
    "import nltk \n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "os.listdir('./hamnspam/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_filenames = [name for name in sorted(os.listdir('./hamnspam/ham')) if len(name) > 20]\n",
    "spam_filenames = [name for name in sorted(os.listdir('./hamnspam/spam')) if len(name) > 20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of ham files:  2551\n",
      "Amount of spam files:  501\n",
      "ham, spam ratio:  5.091816367265469\n",
      "spam, ham ratio:  0.1963935711485692\n"
     ]
    }
   ],
   "source": [
    "print('Amount of ham files: ', len(ham_filenames))\n",
    "print('Amount of spam files: ', len(spam_filenames))\n",
    "print('ham, spam ratio: ', len(ham_filenames)/len(spam_filenames))\n",
    "print('spam, ham ratio: ', len(spam_filenames)/len(ham_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_emails(is_spam, filename):\n",
    "    directory = \"./hamnspam/spam\" if is_spam else \"./hamnspam/ham\"\n",
    "    with open(os.path.join(directory, filename), 'rb') as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
    "\n",
    "ham_email = [load_emails(is_spam = False, filename = name) for name in ham_filenames]\n",
    "spam_email = [load_emails(is_spam = True, filename = name) for name in spam_filenames]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TURNING EMAILS TO TO PLAIN TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    if isinstance(email, str): #if email is a string, it is basically already in plain text so just return the email type\n",
    "        return email \n",
    "    payload = email.get_payload()\n",
    "    if isinstance (payload, list): #if email payload is a list then it means there are multiple emails, so we loop through all the emails in the list and return each email type the \"get email structure\" function\n",
    "        return 'multipart({})'.format(', '.join([get_email_structure(sub_email) for sub_email in payload])) #kinda recursive lol\n",
    "    else: \n",
    "        return email.get_content_type()\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1 #basically just increasing the value count (frequency) by 1\n",
    "    return structures \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_structure = structures_counter(ham_email)\n",
    "spam_structure = structures_counter(spam_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Strip email headers\n",
    "# - Convert to lowercase\n",
    "# - Remove punctuation\n",
    "# - Replace url with \"URLS\"\n",
    "# - Replace numbers with \"NUMBER\"\n",
    "# - Perform stemming (trim word endings with library)\n",
    "\n",
    "class EmailToWords(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, stripHeaders=True, lowercaseConversion=True, punctuationRemoval=True, urlReplace=True, numberReplacement=True, stemming=True):\n",
    "        self.stripHeaders = stripHeaders\n",
    "        self.punctuationRemoval = punctuationRemoval\n",
    "        self.urlReplace = urlReplace\n",
    "        self.numberReplacement = numberReplacement\n",
    "        self.stemming = stemming\n",
    "        self.stemmer = nltk.PorterStemmer()\n",
    "        self.lowercaseConversion = lowercaseConversion\n",
    "    \n",
    "    def html_to_plain(self, email):\n",
    "        try:\n",
    "            soup = BeautifulSoup(email.get_content(), 'html.parser')\n",
    "            return soup.text.replace('\\n\\n', '')\n",
    "        except:\n",
    "            return 'empty'\n",
    "    \n",
    "    def email_to_plain(self, email):\n",
    "        struct = get_email_structure(email)\n",
    "        for part in email.walk():\n",
    "            part_content_type = part.get_content_type()\n",
    "            if part_content_type not in ['text/plain', 'test/html']:\n",
    "                continue \n",
    "            try:\n",
    "                part_content = part.get_content()\n",
    "            except:  #in case of encoding issues\n",
    "                part_content = str(part.get_payload())\n",
    "            if part_content_type == 'text/plain':\n",
    "                return part_content \n",
    "            else:\n",
    "                return self.html_to_plain(part)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_to_words = []\n",
    "        for email in X:\n",
    "            text = self.email_to_plain(email)\n",
    "            if text is None:\n",
    "                text = 'empty'\n",
    "            if self.lowercaseConversion:\n",
    "                text = text.lower()\n",
    "            \n",
    "            if self.punctuationRemoval:\n",
    "                text = text.replace('.', '')\n",
    "                text = text.replace(',', '')\n",
    "                text = text.replace('!', '')\n",
    "                text = text.replace('?', '')\n",
    "            \n",
    "            word_count = Counter(text.split())\n",
    "            if self.stemming:\n",
    "                stemmed_word_count = Counter()\n",
    "                for word, count in word_count.items():\n",
    "                    stemmed_word = self.stemmer.stem(word)\n",
    "                    stemmed_word_count[stemmed_word] += count\n",
    "                word_counts = stemmed_word_count\n",
    "            X_to_words.append(word_count)\n",
    "        return np.array(X_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCountToVector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        total_word_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_word_count[word] += min(count, 10)\n",
    "        self.most_common = total_word_count.most_common()[:self.vocabulary_size]\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(self.most_common)}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        print(len(data))\n",
    "        print(len(rows))\n",
    "        print(len(cols))\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating A Pipeline (A simple one)  \n",
    "\n",
    "Basically Just Preprocessing The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_pipeline = Pipeline([\n",
    "    ('Email To Words', EmailToWords()),\n",
    "    ('Word Count To Vectors', WordCountToVector()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g1f7/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X = np.array(ham_email + spam_email)\n",
    "y = np.array([0] * len(ham_email) + [1] * len(spam_email))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284436\n",
      "284436\n",
      "284436\n"
     ]
    }
   ],
   "source": [
    "X_augumented_train = email_pipeline.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g1f7/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9913974692572478"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
    "score = cross_val_score(log_clf, X_augumented_train, y_train, cv=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80997\n",
      "80997\n",
      "80997\n",
      "0 0\n",
      "Precision: 94.68%\n",
      "Recall: 96.74%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "X_augmented_test = email_pipeline.transform(x_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "log_clf.fit(X_augumented_train, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_augmented_test)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(log_clf, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6 (default, Jan  8 2020, 19:59:22) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4b53b73f77842c109d673a14c5d2f07d384851213ad10cbf54db2905fb9d412"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
